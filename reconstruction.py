# import torch
# import matplotlib.pyplot as plt
# import torchvision.utils as vutils
# from model_imagenet import VQVAE
# from model_mnist import VQVAE
# from dataset import get_imagenet_dataloader, get_mnist_dataloader
# from config import imagenet_config, mnist_config

# # âœ… ì €ì¥ëœ ì²´í¬í¬ì¸íŠ¸ ë¶ˆëŸ¬ì˜¤ê¸°
# def load_model(checkpoint_path):
#     model = VQVAE(
#         in_channels=3,
#         hidden_channels=128,
#         latent_dim=config["latent_dim"],
#         num_residual_layers=2,
#         residual_hidden_channels=32,
#         num_embeddings=config["num_embeddings"],
#         commitment_cost=config["commitment_cost"]
#     ).to(config["device"])

#     model.load_state_dict(torch.load(checkpoint_path, map_location=config["device"]))
#     model.eval()
#     print(f"âœ… ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ '{checkpoint_path}' ë¡œë“œ ì™„ë£Œ!")
#     return model

# # âœ… ì¬êµ¬ì„±ëœ ì´ë¯¸ì§€ ì¶œë ¥ í•¨ìˆ˜
# def show_reconstructed_images(model, data_loader, device, num_images=10, save_path=config["image_path"]):
#     model.eval()
#     original_images = []
#     reconstructed_images = []

#     with torch.no_grad():
#         for images, _ in data_loader:
#             images = images.to(device)
#             reconstructed, _ = model(images)
#             original_images.append(images.cpu())
#             reconstructed_images.append(reconstructed.cpu())
#             if len(original_images) * images.size(0) >= num_images:
#                 break

#     original_images = torch.cat(original_images)[:num_images]
#     reconstructed_images = torch.cat(reconstructed_images)[:num_images]

#     # âœ… ì´ë¯¸ì§€ ì—­ì •ê·œí™” (ImageNet Mean & Std)
#     def denormalize(tensor):
#         mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
#         std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
#         tensor = tensor * std + mean
#         return torch.clamp(tensor, 0, 1)

#     original_images = denormalize(original_images)
#     reconstructed_images = denormalize(reconstructed_images)

#     # âœ… ì›ë³¸ vs ì¬êµ¬ì„± ì´ë¯¸ì§€ ë¹„êµ
#     fig, axes = plt.subplots(nrows=num_images, ncols=2, figsize=(10, num_images * 5))
#     for i in range(num_images):
#         axes[i, 0].imshow(original_images[i].permute(1, 2, 0))
#         axes[i, 0].set_title("Original Image")
#         axes[i, 0].axis("off")
#         axes[i, 1].imshow(reconstructed_images[i].permute(1, 2, 0))
#         axes[i, 1].set_title("Reconstructed Image")
#         axes[i, 1].axis("off")

#     plt.tight_layout()
#     plt.savefig(save_path)
#     print(f"âœ… ì¬êµ¬ì„±ëœ ì´ë¯¸ì§€ê°€ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
#     plt.close()

# # âœ… ì‹¤í–‰ ë¶€ë¶„
# if __name__ == "__main__":
#     print("ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
#     vqvae = load_model(config["checkpoint_path"])
    
#     print("ğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...")
#     _, test_loader = get_imagenet_dataloader()

#     print("ğŸ¨ ì´ë¯¸ì§€ ì¬êµ¬ì„± ì¤‘...")
#     show_reconstructed_images(vqvae, test_loader, config["device"], num_images=10)

# ##################################################################################

# checkpoint_path = config["checkpoint_path"]

# vqvae = VQVAE(
#     channel_in=1,
#     ch=16,
#     latent_channels=config["latent_dim"],
#     code_book_size=config["num_embeddings"],
#     commitment_cost=config["commitment_cost"]
# ).to(config["device"])

# vqvae.load_state_dict(torch.load(checkpoint_path, map_location=config["device"]))
# vqvae.eval()
# print(f"âœ… ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ '{checkpoint_path}' ë¡œë“œ ì™„ë£Œ!")

# # âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
# _, test_loader = get_mnist_dataloader()

# # âœ… ì´ë¯¸ì§€ ë³µì› í•¨ìˆ˜
# def show_reconstructed_images(model, data_loader, device, num_images=10, save_path=config["image_path"]):
#     model.eval()
#     original_images = []
#     reconstructed_images = []

#     with torch.no_grad():
#         for images, _ in data_loader:
#             images = images.to(device)
#             reconstructed, _, _ = model(images)
#             # recon_data, vq_loss, quantized = model(images)
#             # vq_loss, quantized, encoding_indices = model.encode(images)
#             # encoding_indices[0]
            
#             original_images.append(images.cpu())
#             reconstructed_images.append(reconstructed.cpu())
#             if len(original_images) * images.size(0) >= num_images:
#                 break

#     original_images = torch.cat(original_images)[:num_images]
#     reconstructed_images = torch.cat(reconstructed_images)[:num_images]

#     # âœ… ì›ë³¸ ë° ë³µì› ì´ë¯¸ì§€ ì‹œê°í™”
#     fig, axes = plt.subplots(nrows=num_images, ncols=2, figsize=(10, num_images * 2))
#     for i in range(num_images):
#         axes[i, 0].imshow(original_images[i].squeeze(), cmap="gray")
#         axes[i, 0].set_title("Original")
#         axes[i, 0].axis("off")
#         axes[i, 1].imshow(reconstructed_images[i].squeeze(), cmap="gray")
#         axes[i, 1].set_title("Reconstructed")
#         axes[i, 1].axis("off")

#     plt.tight_layout()
#     plt.savefig(save_path)
#     print(f"âœ… ì¬êµ¬ì„±ëœ ì´ë¯¸ì§€ê°€ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
#     plt.close()

# # âœ… ë³µì› ì‹¤í–‰
# show_reconstructed_images(vqvae, test_loader, config["device"])


import torch
import matplotlib.pyplot as plt
from model_imagenet import VQVAE as VQVAE_Imagenet
from model_mnist import VQVAE as VQVAE_MNIST
from dataset import get_imagenet_dataloader, get_mnist_dataloader
from config import imagenet_config, mnist_config

def load_model(checkpoint_path, config, model_class):
    model = model_class(
        in_channels=config.get("channel_in", 3),
        hidden_channels=config.get("hidden_channels", 128),
        latent_dim=config["latent_dim"],
        num_residual_layers=config.get("num_residual_layers", 2),
        residual_hidden_channels=config.get("residual_hidden_channels", 32),
        num_embeddings=config["num_embeddings"],
        commitment_cost=config["commitment_cost"]
    ).to(config["device"])
    model.load_state_dict(torch.load(checkpoint_path, map_location=config["device"]))
    model.eval()
    print(f"âœ… ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ '{checkpoint_path}' ë¡œë“œ ì™„ë£Œ!")
    return model

def show_reconstructed_images(model, data_loader, device, num_images=10, save_path="output.png", is_mnist=False):
    model.eval()
    original_images, reconstructed_images = [], []
    
    with torch.no_grad():
        for images, _ in data_loader:
            images = images.to(device)
            reconstructed, *_ = model(images)
            original_images.append(images.cpu())
            reconstructed_images.append(reconstructed.cpu())
            if len(original_images) * images.size(0) >= num_images:
                break

    original_images = torch.cat(original_images)[:num_images]
    reconstructed_images = torch.cat(reconstructed_images)[:num_images]

    fig, axes = plt.subplots(nrows=num_images, ncols=2, figsize=(10, num_images * 2))
    for i in range(num_images):
        if is_mnist:
            axes[i, 0].imshow(original_images[i].squeeze(), cmap="gray")
            axes[i, 1].imshow(reconstructed_images[i].squeeze(), cmap="gray")
        else:
            axes[i, 0].imshow(original_images[i].permute(1, 2, 0))
            axes[i, 1].imshow(reconstructed_images[i].permute(1, 2, 0))
        axes[i, 0].set_title("Original")
        axes[i, 0].axis("off")
        axes[i, 1].set_title("Reconstructed")
        axes[i, 1].axis("off")

    plt.tight_layout()
    plt.savefig(save_path)
    print(f"âœ… ì¬êµ¬ì„±ëœ ì´ë¯¸ì§€ê°€ '{save_path}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    plt.close()

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser(description="VQ-VAE Reconstruction")
    parser.add_argument("--dataset", type=str, choices=["imagenet", "mnist"], required=True, help="Choose dataset: imagenet or mnist")
    args = parser.parse_args()

    if args.dataset == "imagenet":
        print("ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        vqvae = load_model(imagenet_config["checkpoint_path"], imagenet_config, VQVAE_Imagenet)
        print("ğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...")
        _, test_loader = get_imagenet_dataloader()
        print("ğŸ¨ ì´ë¯¸ì§€ ì¬êµ¬ì„± ì¤‘...")
        show_reconstructed_images(vqvae, test_loader, imagenet_config["device"], num_images=10, save_path=imagenet_config["image_path"], is_mnist=False)
    else:
        print("ğŸ”„ ëª¨ë¸ ë¡œë“œ ì¤‘...")
        vqvae = load_model(mnist_config["checkpoint_path"], mnist_config, VQVAE_MNIST)
        print("ğŸ“¥ ë°ì´í„° ë¡œë“œ ì¤‘...")
        _, test_loader = get_mnist_dataloader()
        print("ğŸ¨ ì´ë¯¸ì§€ ì¬êµ¬ì„± ì¤‘...")
        show_reconstructed_images(vqvae, test_loader, mnist_config["device"], num_images=10, save_path=mnist_config["image_path"], is_mnist=True)
